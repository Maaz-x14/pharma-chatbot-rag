{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "collapsed": true,
        "id": "a_r3AJ-Jpx92",
        "outputId": "a37cebc0-19ca-4685-e933-8234f1d47e64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRAG Pharma Chatbot - Gradio app (single-file)\\n\\nFeatures:\\n- Ask user to upload a PDF (pharma doc)\\n- Extract text, chunk with overlap\\n- Compute embeddings using SentenceTransformers\\n- Store embeddings in FAISS (on-disk optional)\\n- Retrieve top-k relevant chunks\\n- Call Groq LLM (via groq-python or HTTP) to generate final answer using retrieved context\\n- Simple Gradio GUI\\n\\nEnvironment variables expected:\\n- GROQ_API_KEY : your Groq API key\\n- GROQ_MODEL : model id (e.g. \"compound-beta\" or \"llama-3.1-8b-instant\")\\n- SENTENCE_TRANSFORMER_MODEL : (optional) default: \"all-MiniLM-L6-v2\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "RAG Pharma Chatbot - Gradio app (single-file)\n",
        "\n",
        "Features:\n",
        "- Ask user to upload a PDF (pharma doc)\n",
        "- Extract text, chunk with overlap\n",
        "- Compute embeddings using SentenceTransformers\n",
        "- Store embeddings in FAISS (on-disk optional)\n",
        "- Retrieve top-k relevant chunks\n",
        "- Call Groq LLM (via groq-python or HTTP) to generate final answer using retrieved context\n",
        "- Simple Gradio GUI\n",
        "\n",
        "Environment variables expected:\n",
        "- GROQ_API_KEY : your Groq API key\n",
        "- GROQ_MODEL : model id (e.g. \"compound-beta\" or \"llama-3.1-8b-instant\")\n",
        "- SENTENCE_TRANSFORMER_MODEL : (optional) default: \"all-MiniLM-L6-v2\"\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2 sentence-transformers faiss-cpu groq python-dotenv\n",
        "!pip install -U gradio websockets\n",
        "!pip install PyPDF2 pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SzP_-enaqdTI",
        "outputId": "49968c0a-a6e0-41aa-f047-d2f899ccfd0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.34.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (15.0.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "import fitz\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from dataclasses import dataclass\n",
        "from dotenv import load_dotenv\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def semantic_chunk_text(text: str, chunk_size: int = 5, overlap: int = 1) -> List[str]:\n",
        "    \"\"\"Split text into sentence-based chunks for better semantic coherence.\"\"\"\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk = \" \".join(sentences[i:i + chunk_size])\n",
        "        chunks.append(chunk.strip())\n",
        "    return chunks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpfnGdAYqXcy",
        "outputId": "4f6e349f-bf0d-4a23-c7f5-c4d6bf484880"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optional: try to use groq python client if installed, otherwise fallback to simple http call\n",
        "try:\n",
        "    from groq import GroqClient\n",
        "    _HAS_GROQ = True\n",
        "except Exception:\n",
        "    import requests\n",
        "    _HAS_GROQ = False\n"
      ],
      "metadata": {
        "id": "isxZyRfQrHxv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: Read environment directly from notebook secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GROQ_API_KEY = userdata.get('KEY_GEN_AI_HEC_GROQ')\n",
        "GROQ_MODEL = os.getenv(\"GROQ_MODEL\", \"compound-beta\")\n",
        "SENT_MODEL = os.getenv(\"SENTENCE_TRANSFORMER_MODEL\", \"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "h5kBoJVprNim"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simple dataclass to hold chunks and metadata\n",
        "@dataclass\n",
        "class DocChunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    metadata: dict\n",
        "\n"
      ],
      "metadata": {
        "id": "DyqKhXcZrO7V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- PDF extraction ----------\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\")\n",
        "    doc.close()\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "yi2D3NJNrRFT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- Chunking utilities ----------\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"Naive chunker based on characters (safe for PDFs). Adjust chunk_size to tokens for opt.\n",
        "    Returns chunks with overlap.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_len = len(text)\n",
        "    while start < text_len:\n",
        "        end = min(start + chunk_size, text_len)\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "HcWCW5kxrSH1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------- Embeddings and FAISS index ----------\n",
        "\n",
        "class FaissStore:\n",
        "    def __init__(self, dim: int, index_path: str = None):\n",
        "        self.dim = dim\n",
        "        self.index = faiss.IndexFlatIP(self.dim)  # use inner product with normalized vectors for cosine\n",
        "        self.id_map = {}  # mapping from int index -> metadata\n",
        "        self.next_id = 0\n",
        "        self.index_path = index_path\n",
        "\n",
        "    def add(self, vectors: np.ndarray, metadatas: List[dict]):\n",
        "        # vectors should be L2-normalized if using IndexFlatIP for cosine\n",
        "        n_before = self.index.ntotal\n",
        "        self.index.add(vectors)\n",
        "        for i, meta in enumerate(metadatas):\n",
        "            self.id_map[n_before + i] = meta\n",
        "\n",
        "    def search(self, q_vector: np.ndarray, top_k: int = 5, min_score: float = 0.3):\n",
        "        if self.index.ntotal == 0:\n",
        "            return []\n",
        "        D, I = self.index.search(q_vector, top_k)\n",
        "        results = []\n",
        "        for score_list, idx_list in zip(D, I):\n",
        "            for score, idx in zip(score_list, idx_list):\n",
        "                if idx < 0 or score < min_score:\n",
        "                    continue\n",
        "                meta = self.id_map.get(int(idx), {})\n",
        "                results.append((meta, float(score)))\n",
        "        return results\n",
        "\n",
        "\n",
        "    def save(self, path_prefix: str):\n",
        "        faiss.write_index(self.index, path_prefix + \".index\")\n",
        "        with open(path_prefix + \".meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.id_map, f)\n",
        "\n",
        "    def load(self, path_prefix: str):\n",
        "        self.index = faiss.read_index(path_prefix + \".index\")\n",
        "        with open(path_prefix + \".meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            self.id_map = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "svfQLobwrTTU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- LLM (Groq) wrapper ----------\n",
        "\n",
        "class GroqLLM:\n",
        "    def __init__(self, api_key: str, model: str = \"compound-beta\"):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        if _HAS_GROQ:\n",
        "            self.client = GroqClient(api_key=api_key)\n",
        "\n",
        "    def chat_completion(self, system_prompt: str, user_prompt: str, max_tokens: int = 512):\n",
        "        full_prompt = f\"{system_prompt}\\n\\nUser: {user_prompt}\\n\\nAssistant:\"\n",
        "        if _HAS_GROQ:\n",
        "            # Example using groq-python client (wrapper)\n",
        "            response = self.client.chat.create(model=self.model, messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "            ], max_tokens=max_tokens)\n",
        "            return response.choices[0].message.content\n",
        "        else:\n",
        "            # fallback raw HTTP compatible with OpenAI-like endpoint\n",
        "            url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "            body = {\n",
        "                \"model\": self.model,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                \"max_tokens\": max_tokens,\n",
        "            }\n",
        "            r = requests.post(url, headers=headers, json=body)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            # compatible parsing for OpenAI-style response\n",
        "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "cuw_tXperUqY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- Main RAG pipeline helpers ----------\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, emb_model_name: str = SENT_MODEL):\n",
        "        self.emb_model = SentenceTransformer(emb_model_name)\n",
        "        # determine dim by encoding a dummy\n",
        "        v = self.emb_model.encode([\"hello world\"], convert_to_numpy=True)\n",
        "        self.dim = v.shape[1]\n",
        "        self.store = FaissStore(dim=self.dim)\n",
        "        self.llm = GroqLLM(api_key=GROQ_API_KEY, model=GROQ_MODEL)\n",
        "\n",
        "    def process_document(self, file_path: str, batch_size: int = 64):\n",
        "        \"\"\"Reads, chunks, embeds and stores a PDF document.\"\"\"\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "\n",
        "        # --- Chunk into small manageable pieces (character-based for now) ---\n",
        "        chunks = [text[i:i + 500] for i in range(0, len(text), 500)]\n",
        "        print(f\"Total chunks: {len(chunks)}\")\n",
        "\n",
        "        # --- Reset FAISS index and metadata store ---\n",
        "        dim = self.emb_model.get_sentence_embedding_dimension()\n",
        "        # Use cosine similarity: normalize embeddings + IndexFlatIP\n",
        "        self.store.index = faiss.IndexFlatIP(dim)\n",
        "        self.store.id_map = {}\n",
        "        self.store.next_id = 0\n",
        "\n",
        "        # --- Process in batches to prevent memory overflow ---\n",
        "        for i in range(0, len(chunks), batch_size):\n",
        "            batch = chunks[i:i + batch_size]\n",
        "            embeddings = self.emb_model.encode(batch, convert_to_numpy=True)\n",
        "            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "            self.store.index.add(embeddings.astype(\"float32\"))\n",
        "\n",
        "            for chunk in batch:\n",
        "                self.store.id_map[self.store.next_id] = {\"text\": chunk}\n",
        "                self.store.next_id += 1\n",
        "\n",
        "        print(f\"Added {len(chunks)} chunks to FAISS index.\")\n",
        "        return len(chunks)\n",
        "\n",
        "\n",
        "    PHARMA_KEYWORDS = {\"drug\", \"dosage\", \"contraindication\", \"tablet\", \"capsule\", \"injection\", \"pharma\", \"prescription\"}\n",
        "\n",
        "    def query(self, query_text: str, top_k: int = 5, max_tokens: int = 512, min_score: float = 0.3):\n",
        "        # --- Domain filter ---\n",
        "        if not any(kw in query_text.lower() for kw in PHARMA_KEYWORDS):\n",
        "            return \"This question does not appear pharma-related. Please ask a domain-specific query.\", []\n",
        "\n",
        "        # --- Embed & normalize ---\n",
        "        q_emb = self.emb_model.encode([query_text], convert_to_numpy=True)\n",
        "        q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "        # --- Search with threshold ---\n",
        "        results = self.store.search(q_emb.astype('float32'), top_k)\n",
        "        results = [(meta, score) for meta, score in results if score >= min_score]\n",
        "\n",
        "        if not results:\n",
        "            return \"I don’t know. The document does not contain relevant information.\", []\n",
        "\n",
        "        # --- Build context ---\n",
        "        context_pieces = [meta.get(\"text\", \"\") for meta, score in results if meta.get(\"text\", \"\")]\n",
        "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
        "\n",
        "        # --- Strict system prompt ---\n",
        "        system_prompt = (\n",
        "            \"You are a pharma domain assistant. ONLY use the provided context. \"\n",
        "            \"If the context does not contain the answer, respond strictly with: \"\n",
        "            \"'I don’t know. Please check the source document.' \"\n",
        "            \"Do not guess or provide information outside the context.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query_text}\"\n",
        "\n",
        "        answer = self.llm.chat_completion(system_prompt=system_prompt, user_prompt=user_prompt, max_tokens=max_tokens)\n",
        "\n",
        "        # --- Confidence reporting ---\n",
        "        top_score = max(score for _, score in results)\n",
        "        confidence_msg = f\"Top match score = {top_score:.2f} → {'high confidence' if top_score > 0.6 else 'low confidence'}\"\n",
        "\n",
        "        # --- Sources preview ---\n",
        "        sources_text = \"\\n\\nRetrieved chunks:\\n\"\n",
        "        for i, (meta, score) in enumerate(results):\n",
        "            snippet = meta.get(\"text\", \"\")[:200].replace(\"\\n\", \" \")\n",
        "            sources_text += f\"[{i+1}] score={score:.4f} snippet={snippet}...\\n\"\n",
        "\n",
        "        return f\"{answer}\\n\\n{confidence_msg}\\n\\n{sources_text}\", results\n"
      ],
      "metadata": {
        "id": "A-g9AO4TrV6_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- Gradio GUI ----------\n",
        "\n",
        "rag = RAGPipeline()\n",
        "\n",
        "def handle_upload(file_obj):\n",
        "    if file_obj is None:\n",
        "        return \"\", \"No file uploaded\"\n",
        "\n",
        "    # get the file path from the NamedString object\n",
        "    file_path = file_obj.name if hasattr(file_obj, \"name\") else file_obj\n",
        "\n",
        "    # pass directly to RAG pipeline\n",
        "    n_chunks = rag.process_document(file_path)\n",
        "\n",
        "    return f\"Processed PDF, created {n_chunks} chunks.\", \"Upload successful\"\n",
        "\n",
        "\n",
        "def handle_query(question: str):\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "    answer, results = rag.query(question, top_k=5)\n",
        "    # show sources briefly\n",
        "    sources_text = \"\\n\\nRetrieved chunks:\\n\"\n",
        "    for i, (meta, score) in enumerate(results):\n",
        "        sources_text += f\"[{i+1}] score={score:.4f} snippet={meta.get('text','')[:200].replace('\\n',' ')}...\\n\"\n",
        "    return answer + \"\\n\\n\" + sources_text\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"RAG Pharma Chatbot\") as demo:\n",
        "    gr.Markdown(\"# RAG Pharma Chatbot\\nUpload a pharma PDF, then ask domain questions. Uses SentenceTransformer embeddings + FAISS + Groq LLM.\")\n",
        "    with gr.Row():\n",
        "        pdf_in = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "        upload_btn = gr.Button(\"Process PDF\")\n",
        "    status_out = gr.Textbox(label=\"Status\", lines=1)\n",
        "\n",
        "    with gr.Row():\n",
        "        query_in = gr.Textbox(label=\"Ask a question\", placeholder=\"e.g. What is the recommended dosage for drug X?\")\n",
        "        query_btn = gr.Button(\"Ask\")\n",
        "    answer_out = gr.Textbox(label=\"Answer\", lines=10)\n",
        "    confidence_out = gr.Textbox(label=\"Confidence\", lines=1)\n",
        "\n",
        "    query_btn.click(fn=handle_query, inputs=[query_in], outputs=[answer_out, confidence_out])\n",
        "\n",
        "\n",
        "    upload_btn.click(fn=handle_upload, inputs=[pdf_in], outputs=[status_out])\n",
        "    query_btn.click(fn=handle_query, inputs=[query_in], outputs=[answer_out])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(server_name=\"0.0.0.0\", share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "7PLk19BqrXk_",
        "outputId": "f4693017-db31-4afa-a56c-eb02777a80a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://902c167c90f0d115c4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://902c167c90f0d115c4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}